{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import csv\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "3TnA6oZOEj-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_cifar10(dataset_dir, download=True):\n",
        "    \"\"\"Loads the CIFAR10 dataset.\n",
        "    \"\"\"\n",
        "    from torchvision.datasets import CIFAR10\n",
        "    import torchvision.transforms as transforms\n",
        "    from torch.utils.data import ConcatDataset\n",
        "\n",
        "    # Precomputed statistics of CIFAR10 dataset\n",
        "    # Exact values are assumed to be known, but can be estimated with a modest privacy budget\n",
        "    # Opacus wrongly uses CIFAR10_STD = (0.2023, 0.1994, 0.2010)\n",
        "    # This is the _average_ std across all images (see https://github.com/kuangliu/pytorch-cifar/issues/8)\n",
        "    CIFAR10_MEAN = (0.49139968, 0.48215841, 0.44653091)\n",
        "    CIFAR10_STD  = (0.24703223, 0.24348513, 0.26158784)\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
        "    ])\n",
        "\n",
        "    # NB: torchvision checks the integrity of downloaded files\n",
        "    train_dataset = CIFAR10(\n",
        "        root=f\"{dataset_dir}/cifar10\",\n",
        "        train=True,\n",
        "        download=download,\n",
        "        transform=transform\n",
        "    )\n",
        "\n",
        "    test_dataset = CIFAR10(\n",
        "        root=f\"{dataset_dir}/cifar10\",\n",
        "        train=False,\n",
        "        download=download,\n",
        "        transform=transform\n",
        "    )\n",
        "\n",
        "    return ConcatDataset([train_dataset, test_dataset])"
      ],
      "metadata": {
        "id": "QIe_uqcsfCn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(preds: torch.Tensor, labels: torch.Tensor) -> float:\n",
        "    return (preds == labels).mean()\n",
        "\n",
        "# Architecture of shadow model\n",
        "class ShadowNet(nn.Module):\n",
        "    def __init__(self):\n",
        "      super(ShadowNet, self).__init__()\n",
        "      self.shadowCnn = nn.Sequential(\n",
        "            nn.Conv2d(3, 128, kernel_size=8, stride=2, padding=3), nn.Tanh(),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=1),\n",
        "            nn.Conv2d(128, 256, kernel_size=3), nn.Tanh(),\n",
        "            nn.Conv2d(256, 256, kernel_size=3), nn.Tanh(),\n",
        "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "            nn.Flatten()\n",
        "            ,nn.Linear(in_features=6400, out_features=10)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "      output = self.shadowCnn(x)\n",
        "      return output\n",
        "\n",
        "def saveTargetDataset(model,traindataLoaderShader,testDataLoaderShader,data):\n",
        "\n",
        "  model.eval()\n",
        "  sizeData = 50\n",
        "  currentCount = 0\n",
        "  saveFeatureTrain =True\n",
        "  saveFeatureTest =True\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for j,(inputs, target) in enumerate(traindataLoaderShader):\n",
        "      if saveFeatureTrain:\n",
        "        inputs = inputs.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        output = model(inputs)\n",
        "        \n",
        "        features = output.detach().cpu().numpy()\n",
        "        target = target.detach().cpu().numpy()\n",
        "\n",
        "        for count, feature in enumerate(features):\n",
        "          if currentCount < sizeData:\n",
        "            feature = np.append(feature,1)\n",
        "            feature = np.append(feature,target[count])\n",
        "            data.append(feature)\n",
        "            currentCount+=1\n",
        "          else:\n",
        "            saveFeatureTrain = False\n",
        "            break\n",
        "\n",
        "    currentCount = 0\n",
        "    for j,(inputs, target) in enumerate(testDataLoaderShader):\n",
        "      if saveFeatureTest:\n",
        "        inputs = inputs.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        output = model(inputs)\n",
        "        \n",
        "        features = output.detach().cpu().numpy()\n",
        "        target = target.detach().cpu().numpy()\n",
        "\n",
        "        for count, feature in enumerate(features):\n",
        "          if currentCount < sizeData:\n",
        "            feature = np.append(feature,0)\n",
        "            feature = np.append(feature,target[count])\n",
        "            data.append(feature)\n",
        "            currentCount+=1\n",
        "          else:\n",
        "            saveFeatureest = False\n",
        "            break\n",
        "  return data\n",
        "\n",
        "\n",
        "# For infinity epsilon, no DP \n",
        "def trainShadowModels(nModel, shadowData,dirPath):\n",
        "  sTrainingSize = 42000\n",
        "  sEpochs = 50\n",
        "  sBatchSize = 32\n",
        "  sMaxGradNorm = 2.6\n",
        "  sTargetEpsilon = 4.0\n",
        "  sTargetDelta = 1/sTrainingSize\n",
        "  sLR = 0.005\n",
        "  sLrSchedulerGamma = 0.96\n",
        "\n",
        "\n",
        "  for n in range(nModel):\n",
        "    attackData = []\n",
        "    sModel = ShadowNet()\n",
        "    sModel = sModel.to(device)\n",
        "    sCriterion = nn.CrossEntropyLoss()\n",
        "    sOptimizer = optim.SGD(sModel.parameters(), lr=sLR, momentum=0)\n",
        "\n",
        "    shadowTrainData, shadowTestData = train_test_split(shadowData, test_size=0.3, random_state=42)\n",
        "   \n",
        "    traindataLoaderShader = torch.utils.data.DataLoader(shadowTrainData, batch_size=sBatchSize,shuffle = True, num_workers=4,pin_memory = True)\n",
        "    testDataLoaderShader = torch.utils.data.DataLoader(shadowTestData, batch_size=500,shuffle = True, num_workers=4,pin_memory=True)\n",
        "    sScheduler = optim.lr_scheduler.StepLR(sOptimizer, step_size=1, \n",
        "                                          gamma=sLrSchedulerGamma)\n",
        "    for i in range(sEpochs):\n",
        "\n",
        "      sModel.train()\n",
        "\n",
        "      sLosses = []\n",
        "      sTop1Acc = []\n",
        "      dataProcessed =0\n",
        "      for j,(inputs, target) in enumerate(traindataLoaderShader):\n",
        "        inputs = inputs.to(device)\n",
        "        target = target.to(device)\n",
        "        dataProcessed+=len(inputs)\n",
        "\n",
        "        sOptimizer.zero_grad()\n",
        "        sOutput = sModel(inputs)\n",
        "        sLoss = sCriterion(sOutput, target)\n",
        "\n",
        "        sPreds = np.argmax(sOutput.detach().cpu().numpy(), axis=1)\n",
        "        sLabels = target.detach().cpu().numpy()\n",
        "        acc = accuracy(sPreds, sLabels)\n",
        "\n",
        "        sLosses.append(sLoss.item())\n",
        "        sTop1Acc.append(acc)\n",
        "\n",
        "        sLoss.backward()\n",
        "        sOptimizer.step()\n",
        "\n",
        "      sScheduler.step()\n",
        "\n",
        "    # Get the attack dataset\n",
        "    attackData = saveTargetDataset(sModel,traindataLoaderShader,testDataLoaderShader,attackData)\n",
        "    # Test dataset results\n",
        "    print(\"Test results:\")\n",
        "    testShadow(sModel,testDataLoaderShader,sCriterion)\n",
        "\n",
        "    modelName = \"shadowModel\"+str(n)+\".pt\"\n",
        "    print(\"Saving shadow model \",modelName)\n",
        "    torch.save(sModel.state_dict(), os.path.join(dirPath + \"shadowModels/\", modelName))\n",
        "    filename = \"attackData\"+str(n)+\".csv\"\n",
        "    print( \"file: \",filename)\n",
        "    with open(os.path.join(dirPath + \"attackData/\", filename), \"w\", newline='') as f:\n",
        "      for count, data in enumerate(attackData):\n",
        "        csv.writer(f).writerow(data)\n",
        "\n",
        "\n",
        "  return attackData\n"
      ],
      "metadata": {
        "id": "JTehzO3dyjE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def testShadow(model,test_loader,criterion):\n",
        "    model.eval()\n",
        "\n",
        "    losses = []\n",
        "    top1_acc = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, target in tqdm(test_loader):\n",
        "            inputs = inputs.to(device)\n",
        "            target = target.to(device)\n",
        "\n",
        "            output = model(inputs)\n",
        "            loss = criterion(output, target)\n",
        "\n",
        "            preds = np.argmax(output.detach().cpu().numpy(), axis=1)\n",
        "            labels = target.detach().cpu().numpy()\n",
        "\n",
        "            acc = accuracy(preds, labels)\n",
        "\n",
        "            losses.append(loss.item())\n",
        "            top1_acc.append(acc)\n",
        "\n",
        "    top1_avg = np.mean(top1_acc)\n",
        "    loss_avg = np.mean(losses)\n",
        "\n",
        "    print(\n",
        "        f\"Test Loss    : {loss_avg:.6f}\\n\"\n",
        "        f\"Test Accuracy: {top1_avg * 100:.6f}\"\n",
        "    )\n",
        "\n",
        "    return np.mean(top1_acc)"
      ],
      "metadata": {
        "id": "4w3VwECl_8RV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "  # Data directory\n",
        "  dataPath=\"cifar10/\"\n",
        "  if not os.path.isdir(dataPath):\n",
        "    os.mkdir(dataPath)\n",
        "\n",
        "  # Output directory\n",
        "  dirPath=\"output/\"\n",
        "  if not os.path.isdir(dirPath):\n",
        "    os.mkdir(dirPath)\n",
        "\n",
        "  # Attack data directory\n",
        "  if not os.path.isdir(dirPath+\"attackData/\"):\n",
        "    os.mkdir(dirPath+\"attackData/\")\n",
        "  else:\n",
        "    os.rmdir(dirPath+\"attackData/\")\n",
        "    os.mkdir(dirPath+\"attackData/\")\n",
        "\n",
        "  # Shadow models directory\n",
        "  if not os.path.isdir(dirPath+\"shadowModels/\"):\n",
        "    os.mkdir(dirPath+\"shadowModels/\")\n",
        "  else:\n",
        "    os.rmdir(dirPath+\"shadowModels/\")\n",
        "    os.mkdir(dirPath+\"shadowModels/\")\n",
        "\n",
        "  dataset = load_cifar10(dataPath,download = True)\n",
        "  print(len(dataset))\n",
        "  # Train 500 shadow models\n",
        "  attackData = trainShadowModels(500,dataset,dirPath) \n"
      ],
      "metadata": {
        "id": "hJ_X2w9tEEVa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}