{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Membership Inference Competition (MICO) @ IEEE SatML 2023: DP Distinguisher\n",
    "\n",
    "Welcome to the MICO competition!\n",
    "\n",
    "This notebook will walk you through the process of creating and packaging a submission to one of the challenges.\n",
    "\n",
    "Let's start by downloading and extracting the archive for the DP Distinguisher (DDP) challenge. \n",
    "The archive is 87GiB. \n",
    "Downloading, verifying, and extracting it can take a while, so you may want to run the cell below only once.\n",
    "\n",
    "**NOTE**: Public anonymous access to the competition data is disabled. \n",
    "Upon registering for the competition, you will be shown a URL with an embedded bearer token that you must use instead of the URL below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "\n",
    "from torchvision.datasets.utils import download_and_extract_archive\n",
    "\n",
    "url = \"https://membershipinference.blob.core.windows.net/mico/ddp.tar.gz\" \n",
    "filename = \"ddp.tar.gz\"\n",
    "md5 = \"1b9587c2bdf7867e43fb9da345f395eb\"\n",
    "\n",
    "# WARNING: this will download and extract a 87GiB file, if not already present. Please save the file and avoid re-downloading it.\n",
    "try:\n",
    "    download_and_extract_archive(url=url, download_root=os.curdir, extract_root=None, filename=filename, md5=md5, remove_finished=False)\n",
    "except urllib.error.HTTPError as e:\n",
    "    print(e)\n",
    "    print(\"Have you replaced the URL above with the one you got after registering?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "The archive was extracted under the `ddp` folder containing 3 sub-folders, one for each of the scenarios in the challenge:\n",
    "\n",
    "- `cifar10_ddp`     : `CIFAR-10` models (4-layer CNN) trained with DP-SGD and a small privacy budget ($\\epsilon \\approx 4$) \n",
    "- `purchase100_ddp` : `Purchase-100` models (3-layer MLP) trained with DP-SGD and a small privacy budget ($\\epsilon \\approx 4$) \n",
    "- `sst2_ddp`        : `SST-2` models (RoBERTa-Base) fine-tuned with DP-SGD and a small privacy budget ($\\epsilon = \\approx 4$)\n",
    "\n",
    "Each of these folders contains 3 other folders:\n",
    "\n",
    "- `train`: Models with metadata allowing to reconstruct their full training datasets. Use these to develop your attacks without having to train your own models.\n",
    "- `dev`: Models with metadata allowing to reconstruct the sets of challenge examples and non-challenge training examples. Membership predictions for these challenges will be used to evaluate submissions during the competition and update the live scoreboard in CodaLab. \n",
    "- `final`: Models with metadata allowing to reconstruct the sets of challenge examples and non-challenge training examples. Membership predictions for these challenges will be used to evaluate submissions when the competition closes and to determine the final ranking.\n",
    "\n",
    "Each model folder in `train`, `dev`, and `final` contains a `model.pt` file with the model weights (a serialized PyTorch `state_dict`) or `pytorch_model.bin`, `config.json` files with the model weights and configuration (for SST-2). There are 100 models in `train`, and 50 models in each of `dev` and `final`.\n",
    "\n",
    "Models in the `train` folder come with 3 PRNG seeds used to reconstruct the set of member and non-member challenge examples, and the rest of the examples in the training dataset of the model. Additionally (and redundantly), a `solution.csv` file reveals the membership information of the challenge examples.\n",
    "\n",
    "Models in the `dev` and `final` folders contain 2 PRNG seeds used to reconstruct the sets of challenge examples, without revealing which were included in the training dataset, and the set of non-challenge training examples.\n",
    "\n",
    "We provide utilities to reconstruct the different data splits from provided seeds and to load models as classes inheriting from `torch.nn.Module`. If you use TensorFlow, JAX, or any other framework, you can easily convert the models to the appropriate format (e.g. using ONXX).\n",
    "\n",
    "Here's a summary of how the contents are structured:\n",
    "\n",
    "- `cifar10_ddp`\n",
    "  - `train`\n",
    "      - `model_0`\n",
    "        - `model.pt`: Serialized model weights\n",
    "        - `seed_challenge`: PRNG seed used to select a list of 100 challenge examples\n",
    "        - `seed_training`: PRNG seed used to select the non-challenge examples in the training dataset\n",
    "        - `seed_membership`: PRNG seed used to split the set of challenge examples into members and non-members (100 of each)\n",
    "        - `solution.csv`: Membership information of the challenge examples (`1` for member, `0` for non-member)\n",
    "      - ...\n",
    "  - `dev`\n",
    "      - `model_100`\n",
    "        - `model.pt`\n",
    "        - `seed_challenge`\n",
    "        - `seed_training`\n",
    "      - ...\n",
    "  - `final`\n",
    "    - `model_150`\n",
    "      - `model.pt`\n",
    "      - `seed_challenge`\n",
    "      - `seed_training`\n",
    "    - ...\n",
    "- `purchase100_ddp`\n",
    "  - ...\n",
    "- `sst2_ddp`\n",
    "  - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "\n",
    "Your task as a competitor is to produce, for each model in `dev` and `final`, a CSV file listing your confidence scores (values between 0 and 1) for the membership of the challenge examples. You must save these scores in a `prediction.csv` file and place it in the same folder as the corresponding model. A submission to the challenge is an an archive containing just these `prediction.csv` files.\n",
    "\n",
    "**You must submit predictions for both `dev` and `final` when you submit to CodaLab.**\n",
    "\n",
    "In the following, we will show you how to compute predictions from a basic membership inference attack and package them as a submission archive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import csv\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from mico_competition import ChallengeDataset, load_sst2, load_cifar10, load_purchase100, load_model\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "assert torch.cuda.is_available(), \"CUDA is not available; the below would only work with CUDA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHALLENGE = \"ddp\"\n",
    "LEN_TRAINING = { 'cifar10_ddp': 50000, 'purchase100_ddp': 150000, 'sst2_ddp': 67349 }\n",
    "LEN_CHALLENGE = 100\n",
    "\n",
    "scenarios = os.listdir(CHALLENGE)\n",
    "phases = ['dev', 'final','train']\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "for scenario in tqdm(scenarios, desc=\"scenario\"):\n",
    "    if scenario == \"cifar10_ddp\":\n",
    "        dataset = load_cifar10(dataset_dir=\"/data\")\n",
    "        datasetName = \"cifar10\"\n",
    "        batch_size = 2*LEN_CHALLENGE\n",
    "    elif scenario == \"purchase100_ddp\":\n",
    "        dataset = load_purchase100(dataset_dir=\"/data\")\n",
    "        datasetName = \"purchase100\"\n",
    "        batch_size = 2*LEN_CHALLENGE\n",
    "    else:\n",
    "        assert scenario == \"sst2_ddp\"\n",
    "        dataset = load_sst2()\n",
    "        datasetName = \"sst2\"\n",
    "        batch_size = 10\n",
    "\n",
    "    for phase in tqdm(phases, desc=\"phase\"):\n",
    "        root = os.path.join(CHALLENGE, scenario, phase)\n",
    "        for model_folder in tqdm(sorted(os.listdir(root), key=lambda d: int(d.split('_')[1])), desc=\"model\"):\n",
    "            path = os.path.join(root, model_folder)\n",
    "            challenge_dataset = ChallengeDataset.from_path(path, dataset=dataset, len_training=LEN_TRAINING[scenario])\n",
    "            challenge_points = challenge_dataset.get_challenges()\n",
    "\n",
    "            # This is where you plug in your membership inference attack\n",
    "            # As an example, here is a simple loss threshold attack\n",
    "\n",
    "            # Loss Threshold Attack\n",
    "            model = load_model(datasetName, path)\n",
    "            challenge_dataloader = torch.utils.data.DataLoader(challenge_points, batch_size=batch_size)\n",
    "\n",
    "            model = model.to(torch.device('cuda'))\n",
    "            predictions = []\n",
    "            for batch in challenge_dataloader:\n",
    "                if scenario == \"sst2_ddp\":\n",
    "                    labels = batch['label'].to(torch.device('cuda'))\n",
    "                    tokenizedSequences = tokenizer(batch['sentence'], return_tensors=\"pt\", padding=\"max_length\", max_length=67)\n",
    "                    tokenizedSequences = tokenizedSequences.to(torch.device('cuda'))\n",
    "                    output = model(**tokenizedSequences).logits\n",
    "                else:\n",
    "                    features, labels = batch\n",
    "                    features = features.to(torch.device('cuda'))\n",
    "                    labels = labels.to(torch.device('cuda'))\n",
    "                    output = model(features)\n",
    "\n",
    "                batch_predictions = -criterion(output, labels).detach().cpu().numpy()\n",
    "                predictions.extend(batch_predictions)\n",
    "\n",
    "            # Normalize to unit interval\n",
    "            min_prediction = np.min(predictions)\n",
    "            max_prediction = np.max(predictions)\n",
    "            predictions = (predictions - min_prediction) / (max_prediction - min_prediction)\n",
    "\n",
    "            assert np.all((0 <= predictions) & (predictions <= 1))\n",
    "\n",
    "            with open(os.path.join(path, \"prediction.csv\"), \"w\") as f:\n",
    "                 csv.writer(f).writerow(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring\n",
    "\n",
    "Let's see how the attack does on `train`, for which we have the ground truth. \n",
    "When preparing a submission, you can use part of `train` to develop an attack and a held-out part to evaluate your attack. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mico_competition.scoring import tpr_at_fpr, score, generate_roc, generate_table\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "FPR_THRESHOLD = 0.1\n",
    "\n",
    "all_scores = {}\n",
    "phases = ['train']\n",
    "\n",
    "for scenario in tqdm(scenarios, desc=\"scenario\"): \n",
    "    all_scores[scenario] = {}    \n",
    "    for phase in tqdm(phases, desc=\"phase\"):\n",
    "        predictions = []\n",
    "        solutions  = []\n",
    "\n",
    "        root = os.path.join(CHALLENGE, scenario, phase)\n",
    "        for model_folder in tqdm(sorted(os.listdir(root), key=lambda d: int(d.split('_')[1])), desc=\"model\"):\n",
    "            path = os.path.join(root, model_folder)\n",
    "            predictions.append(np.loadtxt(os.path.join(path, \"prediction.csv\"), delimiter=\",\"))\n",
    "            solutions.append(np.loadtxt(os.path.join(path, \"solution.csv\"),   delimiter=\",\"))\n",
    "\n",
    "        predictions = np.concatenate(predictions)\n",
    "        solutions = np.concatenate(solutions)\n",
    "        \n",
    "        scores = score(solutions, predictions)\n",
    "        all_scores[scenario][phase] = scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the ROC curve for the attack and see how the attack performed on different metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "for scenario in scenarios:\n",
    "    fpr = all_scores[scenario]['train']['fpr']\n",
    "    tpr = all_scores[scenario]['train']['tpr']\n",
    "    fig = generate_roc(fpr, tpr)\n",
    "    fig.suptitle(f\"{scenario}\", x=-0.1, y=0.5)\n",
    "    fig.tight_layout(pad=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "for scenario in scenarios:\n",
    "    print(scenario)\n",
    "    scores = all_scores[scenario]['train']\n",
    "    scores.pop('fpr', None)\n",
    "    scores.pop('tpr', None)\n",
    "    display(pd.DataFrame([scores]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packaging the submission\n",
    "\n",
    "Now we can store the predictions into a zip file, which you can submit to CodaLab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "phases = ['dev', 'final']\n",
    "\n",
    "with zipfile.ZipFile(\"predictions_ddp.zip\", 'w') as zipf:\n",
    "    for scenario in tqdm(scenarios, desc=\"scenario\"): \n",
    "        for phase in tqdm(phases, desc=\"phase\"):\n",
    "            root = os.path.join(CHALLENGE, scenario, phase)\n",
    "            for model_folder in tqdm(sorted(os.listdir(root), key=lambda d: int(d.split('_')[1])), desc=\"model\"):\n",
    "                path = os.path.join(root, model_folder)\n",
    "                file = os.path.join(path, \"prediction.csv\")\n",
    "                if os.path.exists(file):\n",
    "                    zipf.write(file)\n",
    "                else:\n",
    "                    raise FileNotFoundError(f\"`prediction.csv` not found in {path}. You need to provide predictions for all challenges\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('mico-competition')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1c823568a0650a753a55947c22141ec594c2fc02bd68b5a71e505ecc57f17796"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
